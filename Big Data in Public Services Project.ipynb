{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Anaconda3.1\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Luca\\\\Desktop\\\\BICOCCA\\\\public\\\\Mercorio\\\\progetto\\\\Files\")\n",
    "\n",
    "%run -i \"libraries.py\"\n",
    "%run -i \"functions.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. File reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Luca\\\\Desktop\\\\BICOCCA\\\\public\\\\Mercorio\\\\progetto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge file not found, loading every .csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [04:07<00:00, 27.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comments: 2040273\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"NYT-merge.csv\", sep=\";\", encoding=\"UTF-8\")\n",
    "    data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(\"Loaded comments: %s\" %(len(data)))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Merge file not found, loading every .csv\")\n",
    "    \n",
    "    articles_path = \"C:\\\\Users\\\\Luca\\\\Desktop\\\\BICOCCA\\\\public\\\\Mercorio\\\\progetto\\\\Articles\\\\*.csv\"\n",
    "    comments_path = \"C:\\\\Users\\\\Luca\\\\Desktop\\\\BICOCCA\\\\public\\\\Mercorio\\\\progetto\\\\Comments\\\\*.csv\"\n",
    "    articles_list = glob.glob(articles_path)\n",
    "    comments_list = glob.glob(comments_path)\n",
    "\n",
    "    \n",
    "    # Loading data\n",
    "    articles = get_articles(articles_path, articles_list)\n",
    "    comments = get_comments(comments_path, comments_list)\n",
    "\n",
    "    # Merge\n",
    "    data = pd.merge(articles, comments, left_on=\"artID\", right_on=\"comID\", how=\"left\").drop(\"comID\", axis=1)\n",
    "\n",
    "    \n",
    "    data = data.dropna() # around 100k rows do not match any article ID\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(\"index\", axis=1)\n",
    "    data = data[data.Keywords != ''] # remove empty keywords\n",
    "    data = data.drop_duplicates(subset=[\"Comments\"], keep=\"first\")\n",
    "    print(\"Loaded comments: %s\" %(len(data)))\n",
    "\n",
    "    data.to_csv(\"NYT-merge.csv\", sep=\";\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.128*\"Trump, Donald J\" + 0.113*\"United States Politics and Government\" + 0.063*\"Russia\" + 0.043*\"Presidential Election of 2016\" + 0.037*\"Federal Bureau of Investigation\" + 0.029*\"United States International Relations\" + 0.028*\"Comey, James B\" + 0.026*\"Cyberwarfare and Defense\" + 0.023*\"Russian Interference in 2016 US Elections and Ties to Trump Associates\" + 0.019*\"Special Prosecutors (Independent Counsel)\"')\n",
      "(1, '0.099*\"Trump, Donald J\" + 0.082*\"United States Politics and Government\" + 0.042*\"Comey, James B\" + 0.039*\"Federal Bureau of Investigation\" + 0.037*\"Global Warming\" + 0.032*\"United Nations Framework Convention on Climate Change\" + 0.020*\"Crossword Puzzles\" + 0.018*\"United States International Relations\" + 0.017*\"Greenhouse Gas Emissions\" + 0.015*\"Clinton, Hillary Rodham\"')\n",
      "(2, '0.045*\"Trump, Donald J\" + 0.039*\"Politics and Government\" + 0.029*\"United States International Relations\" + 0.019*\"France\" + 0.018*\"United States Politics and Government\" + 0.017*\"Macron, Emmanuel (1977- )\" + 0.017*\"Elections\" + 0.015*\"European Union\" + 0.015*\"Israel\" + 0.013*\"Le Pen, Marine\"')\n",
      "(3, '0.145*\"United States Politics and Government\" + 0.140*\"Trump, Donald J\" + 0.048*\"Republican Party\" + 0.029*\"Health Insurance and Managed Care\" + 0.028*\"Obama, Barack\" + 0.027*\"House of Representatives\" + 0.021*\"Patient Protection and Affordable Care Act (2010)\" + 0.019*\"Ryan, Paul D Jr\" + 0.018*\"United States International Relations\" + 0.013*\"Appointments and Executive Changes\"')\n",
      "(4, '0.070*\"Gun Control\" + 0.070*\"School Shootings and Armed Attacks\" + 0.053*\"Parkland, Fla, Shooting (2018)\" + 0.032*\"National Rifle Assn\" + 0.019*\"Demonstrations, Protests and Riots\" + 0.018*\"Women and Girls\" + 0.015*\"Trump, Donald J\" + 0.015*\"Mental Health and Disorders\" + 0.013*\"Firearms\" + 0.011*\"Youth\"')\n",
      "(5, '0.059*\"Trump, Donald J\" + 0.042*\"United States Politics and Government\" + 0.025*\"Fox News Channel\" + 0.025*\"Deferred Action for Childhood Arrivals\" + 0.023*\"Sexual Harassment\" + 0.019*\"Ethics and Official Misconduct\" + 0.018*\"#MeToo Movement\" + 0.017*\"Illegal Immigration\" + 0.015*\"New York City\" + 0.013*\"Immigration and Emigration\"')\n",
      "(6, '0.122*\"Trump, Donald J\" + 0.118*\"United States Politics and Government\" + 0.043*\"Republican Party\" + 0.039*\"House of Representatives\" + 0.033*\"Senate\" + 0.029*\"Patient Protection and Affordable Care Act (2010)\" + 0.027*\"Health Insurance and Managed Care\" + 0.023*\"Immigration and Emigration\" + 0.021*\"Law and Legislation\" + 0.016*\"Democratic Party\"')\n",
      "(7, '0.029*\"Social Media\" + 0.026*\"Facebook Inc\" + 0.026*\"Supreme Court (US)\" + 0.021*\"Television\" + 0.017*\"Trump, Donald J\" + 0.015*\"Children and Childhood\" + 0.015*\"News and News Media\" + 0.014*\"Law and Legislation\" + 0.014*\"Data-Mining and Database Marketing\" + 0.014*\"Cambridge Analytica\"')\n",
      "(8, '0.080*\"United States Politics and Government\" + 0.061*\"Trump, Donald J\" + 0.027*\"Democratic Party\" + 0.026*\"Elections, House of Representatives\" + 0.023*\"Women and Girls\" + 0.021*\"Labor and Jobs\" + 0.015*\"Midterm Elections (2018)\" + 0.015*\"Republican Party\" + 0.015*\"Global Warming\" + 0.014*\"United States Economy\"')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ldamodel = gensim.models.LdaMulticore.load(\"model-9topic.gensim\")\n",
    "    print_topics=10\n",
    "    topics = ldamodel.print_topics(num_words=print_topics)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: cannot load LDA model, file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found, running 'lda_to_topic' function to assign topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2040273/2040273 [00:17<00:00, 119280.07it/s]\n"
     ]
    }
   ],
   "source": [
    "data_newkey = lda_assign(\"NYT-9topic.csv\", data, ldamodel)\n",
    "#print(np.unique(data_newkey[\"Keywords\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2040273/2040273 [02:09<00:00, 15728.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this project makes me happy to be a year times subscriber continue to innovate across all platforms please\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2040273/2040273 [02:35<00:00, 13154.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2040273/2040273 [00:24<00:00, 84466.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [08:04<00:00, 164.54s/it]\n"
     ]
    }
   ],
   "source": [
    "data_prep = temp = data_newkey\n",
    "\n",
    "text = pre_processing(temp[\"Comments\"], stopwords=0)\n",
    "data_newkey.drop(\"Comments\", axis=1, inplace=True)\n",
    "data_newkey[\"Comments\"] = text\n",
    "data_newkey.to_csv(\"NYT-NewKeywords.csv\", sep=\";\", encoding=\"UTF-8\") # Saving\n",
    "print(data_newkey[\"Comments\"].iloc[0])\n",
    "\n",
    "text_stop = pre_processing(temp[\"Comments\"], stopwords=1)\n",
    "data_prep.drop(\"Comments\", axis=1, inplace=True)\n",
    "data_prep[\"Comments\"] = text_stop\n",
    "\n",
    "# Remove short comments\n",
    "data_prep = remove_short(data_prep, \"Comments\", 9) # Remove\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_prep[\"Comments\"] = lemmatization(data_prep[\"Comments\"])\n",
    "\n",
    "# Remove duplicate comments\n",
    "data_prep = data_prep.drop_duplicates(subset=[\"Comments\"], keep=\"first\")\n",
    "\n",
    "#print(data_prep.head()); print(\"\\n\", Counter(list(data_prep[\"Keywords\"])))\n",
    "\n",
    "\n",
    "data_prep.to_csv(\"NYT-Preprocessing.csv\", sep=\";\", encoding=\"UTF-8\") # Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(Social)_Media', 'Environment', 'Ethics', 'Guns', 'International',\n",
       "       'Laws', 'News', 'US_Elections_2016', 'US_Politics'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_newkey = pd.read_csv(\"NYT-NewKeywords.csv\", sep=\";\", encoding=\"UTF-8\")\n",
    "data_newkey = data_newkey.dropna()\n",
    "#np.unique(data_newkey[\"Keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_newkey[\"Comments\"], \n",
    "                                                    data_newkey[\"Keywords\"],\n",
    "                                                    stratify=data_newkey[\"Keywords\"],\n",
    "                                                    test_size=0.3, random_state=0)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_vect = TfidfVectorizer(min_df=10, ngram_range=(2,2))\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.25, fit_prior=False).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Constructor for sentiment\n",
    "afinn = Afinn()\n",
    "\n",
    "# Full list of comments\n",
    "com_list = list(data_newkey[\"Comments\"])\n",
    "whole_list = [word for row in com_list for word in row.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1161192/1161192 [00:03<00:00, 359635.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collecting the tags for each words\n",
    "word_tags = defaultdict(Counter)\n",
    "for word, pos in tqdm(brown.tagged_words(tagset='universal')):\n",
    "    word_tags[word][pos] += 1\n",
    "    \n",
    "\n",
    "brown_tags_words = []\n",
    "for sent in brown.tagged_sents(tagset='universal'): # Simplified version\n",
    "    brown_tags_words.append((\"START\", \"START\"))\n",
    "    brown_tags_words.extend([(tag[:2], word) for (word, tag) in sent])\n",
    "    brown_tags_words.append((\"END\", \"END\"))\n",
    "\n",
    "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
    "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "\n",
    "brown_tags = [tag for (tag, word) in brown_tags_words]\n",
    "cfd_tags = nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "    \n",
    "distinct_tags = set(brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence):\n",
    "    viterbi, backpointer = ([] for i in range(2))\n",
    "    first_viterbi, first_backpointer = ({} for i in range(2))\n",
    "    \n",
    "    for tag in distinct_tags:\n",
    "        if tag == \"START\": continue\n",
    "        first_viterbi[tag] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob(sentence[0])\n",
    "        first_backpointer[tag] = \"START\"\n",
    "        \n",
    "    viterbi.append(first_viterbi)\n",
    "    backpointer.append(first_backpointer)\n",
    "    currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[tag])\n",
    "    \n",
    "    \n",
    "    for wordindex in range(1, len(sentence)):\n",
    "        this_viterbi, this_backpointer = ({} for i in range(2))\n",
    "        prev_viterbi = viterbi[-1]\n",
    "        \n",
    "        for tag in distinct_tags:\n",
    "            if tag == \"START\": continue\n",
    "            best_previous = max(prev_viterbi.keys(),\n",
    "                                key = lambda prevtag: \\\n",
    "            prev_viterbi[prevtag] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "\n",
    "            this_viterbi[tag] = prev_viterbi[best_previous] * \\\n",
    "            cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex])\n",
    "            this_backpointer[tag] = best_previous\n",
    "\n",
    "        currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[tag])\n",
    "        viterbi.append(this_viterbi)\n",
    "        backpointer.append(this_backpointer)\n",
    "    \n",
    "    \n",
    "    prev_viterbi = viterbi[-1]\n",
    "    best_previous = max(prev_viterbi.keys(),\n",
    "                        key = lambda prevtag: prev_viterbi[prevtag] * cpd_tags[prevtag].prob(\"END\"))\n",
    "\n",
    "    prob_tagsequence = prev_viterbi[best_previous] * cpd_tags[best_previous].prob(\"END\")\n",
    "\n",
    "    # best tagsequence: we store this in reverse for now, will invert later\n",
    "    best_tagsequence = [\"END\", best_previous]\n",
    "    # invert the list of backpointers\n",
    "    backpointer.reverse()\n",
    "\n",
    "    current_best_tag = best_previous\n",
    "    for bp in backpointer:\n",
    "        best_tagsequence.append(bp[current_best_tag])\n",
    "        current_best_tag = bp[current_best_tag]\n",
    "\n",
    "    best_tagsequence.reverse()\n",
    "    \n",
    "    return(\" \".join(best_tagsequence[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def category(model, sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence this function estimate the probability of belonging to one class.\n",
    "    It return a dataframe with classes and estimated probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    # https://datascienceplus.com/multi-class-text-classification-with-scikit-learn/\n",
    "    if type(sentence) == list:\n",
    "         sentence = \" \".join(sentence)\n",
    "    predictions = (model.predict_proba(count_vect.transform([sentence])))\n",
    "    ref_classes = (model.classes_).tolist()\n",
    "    predictions = [value for lista in predictions.tolist() for value in lista]\n",
    "    \n",
    "    df_prob_class =  pd.DataFrame({'Pred': predictions, 'class': ref_classes})\n",
    "    df_prob_class = df_prob_class.sort_values([\"Pred\"], ascending=False)\n",
    "    df_prob_class = df_prob_class.reset_index().drop(\"index\", axis=1)\n",
    "    \n",
    "    print(tabulate(df_prob_class, headers=\"keys\", tablefmt=\"psql\", showindex=False))\n",
    "    \n",
    "    return(df_prob_class)\n",
    "\n",
    "\n",
    "def random_select(length, comm_list):\n",
    "    pos = np.random.choice(len(comm_list))\n",
    "    sentence = (comm_list[pos:pos+length])\n",
    "    sentence = [el for el in list(sentence) if len(el) > 0]\n",
    "    \n",
    "    temp_sent = pre_processing(sentence, stopwords=1)\n",
    "    temp_sent = \" \".join(temp_sent)\n",
    "    \n",
    "    temp_df = pd.DataFrame([temp_sent], columns=[\"Sentence\"])\n",
    "    \n",
    "    temp_sent = lemmatization(temp_df[\"Sentence\"])\n",
    "    \n",
    "    temp_sent = [el for el in list(temp_sent) if len(el) > 0]\n",
    "    temp_sent = ' '.join(temp_sent)\n",
    "    \n",
    "    return(sentence, temp_sent)\n",
    "\n",
    "\n",
    "def follow(topic_df, sentence, end, n_match):\n",
    "    data = pd.DataFrame()\n",
    "    afinn = Afinn()\n",
    "    follow_freq = {}\n",
    "    \n",
    "    while len(follow_freq) < n_match: # I need at least a given number of following set of words\n",
    "        follow_freq = {}\n",
    "        string_sentence = \" \".join(sentence[-end:])\n",
    "        \n",
    "        for comment in topic_df[\"Comments\"]:\n",
    "            if string_sentence in comment: # I only need comment with the sentence \n",
    "                comment_list = comment.split(string_sentence)\n",
    "\n",
    "                for part in comment_list[1:]:\n",
    "                    part = part.split(\" \")\n",
    "                    if len(part) > end: # I need at least end following words (the first element is a blank space)\n",
    "                        part = part[1:]\n",
    "                        following = \"\"\n",
    "\n",
    "                        for i in range(end):\n",
    "                            following = following + part[i] + \" \"\n",
    "                        \n",
    "                        following = following[:len(following)-1] # remove the last \"\"\n",
    "                        \n",
    "                        if following in follow_freq:\n",
    "                            follow_freq[following] += 1\n",
    "                        else:\n",
    "                            follow_freq[following] = 1\n",
    "\n",
    "        if len(follow_freq) < n_match:\n",
    "            end -= 1\n",
    "    \n",
    "    follow_freq = Counter(follow_freq).most_common()\n",
    "    \n",
    "    # Check if null sentiment\n",
    "    max_sent = 0\n",
    "    for cont in range(len(follow_freq)):\n",
    "        temp_follow = follow_freq[cont][0].split(\" \")\n",
    "        for i in range(len(temp_follow)):\n",
    "            test_sent = afinn.score(temp_follow[i])\n",
    "            if test_sent > max_sent:\n",
    "                max_sent = test_sent\n",
    "    \n",
    "    if max_sent == 0:\n",
    "        print(\"Warning! Adding word with a null sentiment\")\n",
    "        \n",
    "    return(follow_freq, end, max_sent)  \n",
    "\n",
    "\n",
    "def words_dataframe_light(topic_df, sentence, alpha, method, step, n_match, end_seq):\n",
    "    if step == 0:\n",
    "        end = len(sentence)\n",
    "    else:\n",
    "        end = end_seq\n",
    "        \n",
    "    status = \"not ok\"\n",
    "    \n",
    "    while status == \"not ok\":\n",
    "        (follow_freq, end, max_sent) = follow(topic_df, sentence, end, n_match)\n",
    "        \n",
    "        if method == 1: # If what I care most is the sentence's meaning\n",
    "            if len(follow_freq) > 0:\n",
    "                status = \"ok\"\n",
    "        if method == 0: # If what I care most is the sentence's sentiment\n",
    "            if max_sent == 0:\n",
    "                if end > 2:\n",
    "                    end -= 1\n",
    "                else:\n",
    "                    print(\"No more words can be added for the given sentence!\")\n",
    "                    break\n",
    "            else:\n",
    "                status = \"ok\"\n",
    "              \n",
    "            \n",
    "    \"\"\"\n",
    "    Follow freq is a tuple with \"list of words\", \"freq\".\n",
    "    I create as many lists as the number of words in \"list of words\" which is exactly the end value\n",
    "    I must have two more lists: one for store the freq count and one for the sentiment\n",
    "    Words in the \"list of words\" which also appears at the end of the sentence are not considered\n",
    "    With what I found, I create a new index value which is a combination between the freq count and the sentiment\n",
    "    \"\"\"        \n",
    "    lists = [[] for _ in range(end+2)]\n",
    "    big_list = [el for el in lists]\n",
    "    \n",
    "    follow_count = []\n",
    "    afinn_score = []\n",
    "    for cont in range(len(follow_freq)):\n",
    "        temp_follow = follow_freq[cont][0].split(\" \")\n",
    "        sent_score = 0\n",
    "        for i in range(len(temp_follow)):\n",
    "            if temp_follow[i] not in sentence[-end:]: # I try not to add a word which is already at the end of the sentence\n",
    "                try:\n",
    "                    big_list[i].append(temp_follow[i])\n",
    "                except IndexError:\n",
    "                    break\n",
    "                sent_score = sent_score + afinn.score(temp_follow[i])\n",
    "            else:\n",
    "                big_list[i].append(\" \")\n",
    "                sent_score += 0\n",
    "                \n",
    "        big_list[end].append(follow_freq[cont][1])\n",
    "        big_list[end+1].append(sent_score)\n",
    "        try:\n",
    "            if len(big_list[end]) != len(big_list[i]): # It happens when a word in also at the end of the sentence\n",
    "                del (big_list[end][-1], big_list[end+1][-1]) \n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    for i in range(len(big_list[end])):\n",
    "        if type(big_list[end][i]) == str:\n",
    "            big_list[end][i] = 0\n",
    "        if type(big_list[end+1][i]) == str:\n",
    "            big_list[end+1][i] = 0\n",
    "\n",
    "    (max_count, max_sent) = (max(big_list[end]), max(big_list[end+1]))\n",
    "    count_stand = [big_list[end][cont]/max_count for cont in range(len(big_list[end]))]\n",
    "    \n",
    "    if max_sent == 0:\n",
    "        max_sent = 0.01\n",
    "    \n",
    "    sent_stand = [big_list[end+1][cont]/max_sent for cont in range(len(big_list[end+1]))]\n",
    "    best_pick = [(count_stand[cont]*(sent_stand[cont] + alpha)) for cont in range(len(big_list[end]))]\n",
    "            \n",
    "    # Creating names from the df\n",
    "    variables = [\"Follow\" + str(i+1) for i in range(end)]\n",
    "    new_vars = [\"Count\", \"Sentiment\", \"Count_stand\", \"Sent_stand\", \"Best_pick\"]\n",
    "    for var in new_vars:\n",
    "        variables.append(var)\n",
    "    \n",
    "    big_list.append(count_stand)\n",
    "    big_list.append(sent_stand)\n",
    "    big_list.append(best_pick)\n",
    "    diz = {}\n",
    "    for i in range(len(big_list)):\n",
    "        diz[variables[i]] = big_list[i]\n",
    "    \n",
    "\n",
    "    datt = pd.DataFrame(diz, columns=diz.keys())\n",
    "    \n",
    "    return(datt, end, max_sent)  \n",
    "\n",
    "\n",
    "\n",
    "def final_function_light(df, model, length, comment_list, alpha, end_seq, n_match, method):\n",
    "    (sentence, prep_sentence) = random_select(length, comment_list)\n",
    "    \n",
    "    threshold = int(input(\"Type a number as the sentiment upper bound \"))\n",
    "    print(\"Sentiment threshold:\", str(threshold))\n",
    "    \n",
    "    sentiment = 0\n",
    "    step = 0\n",
    "    \n",
    "    # predict the class\n",
    "    data_class = category(model, prep_sentence)\n",
    "    print(\"{\", sentence, \"}\\t predicted class:\", data_class[\"class\"].iloc[0])\n",
    "    globals()[\"data_%s\" %data_class[\"class\"].iloc[0]] = df.loc[df[\"Keywords\"] == data_class[\"class\"].iloc[0]]\n",
    "    \n",
    "    \n",
    "    while abs(sentiment) < abs(threshold):\n",
    "        print(\"{\", \" \".join(sentence), \"}\\t sentiment score:\", afinn.score(\" \".join(sentence)), \"\\n\")\n",
    "        \n",
    "        # find the most common following words\n",
    "        (data, end, max_sent) = words_dataframe_light(globals()[\"data_%s\" %data_class[\"class\"].iloc[0]], \n",
    "                                                      sentence, alpha=alpha, n_match=n_match, step=step, method=method,\n",
    "                                                      end_seq=end_seq)\n",
    "        data = data.reset_index(drop=True)\n",
    "        #data.drop(\"index\",axis=1,inplace=True)\n",
    "        \n",
    "        \n",
    "        if threshold > 0:\n",
    "            data = data[data[\"Best_pick\"] > 0]\n",
    "            data = data.sort_values(by=['Best_pick'], ascending=False)\n",
    "        else:\n",
    "            data = data[data[\"Best_pick\"] < 0]\n",
    "            data = data.sort_values(by=['Best_pick'], ascending=True)\n",
    "        \n",
    "        \n",
    "        # Tag sequence\n",
    "        tag_seq_list = []\n",
    "        tag_seq = \"\"\n",
    "        for n in range(len(data)):\n",
    "            for i in range(end):\n",
    "                try:\n",
    "                    tag_seq = tag_seq + (list(word_tags[data[\"Follow\" + str(i+1)].iloc[n]]))[0] + \" \"\n",
    "                except IndexError:\n",
    "                    tag_seq = tag_seq + \"None\" + \" \"\n",
    "            tag_seq = tag_seq[:-1]\n",
    "            tag_seq_list.append(tag_seq)\n",
    "        data[\"Tags\"] = tag_seq_list\n",
    "        \n",
    "        best_tag_seq = viterbi(sentence[-end:])\n",
    "        try:\n",
    "            data_tag = data.loc[data[\"Tags\"] == best_tag_seq]\n",
    "        except TypeError:\n",
    "            pass\n",
    "        if len(data_tag) > 0:\n",
    "            data = data_tag\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Update sentence\n",
    "        sentence = \" \".join(sentence)\n",
    "        for i in range(end):\n",
    "            if data[\"Follow\" + str(i+1)].iloc[0] != \"\":\n",
    "                sentence = sentence + \" \" + data[\"Follow\" + str(i+1)].iloc[0]\n",
    "        print(sentence)\n",
    "                \n",
    "        sentence = sentence.split(\" \")\n",
    "        sentiment = afinn.score(\" \".join(sentence))\n",
    "        step += 1\n",
    "        \n",
    "    print(\"New sentence: {\", \" \".join(sentence), \"}\\t score:\", afinn.score(\" \".join(sentence)))\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 20063.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1486.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a number as the sentiment upper bound 10\n",
      "Sentiment threshold: 10\n",
      "+-----------+-------------------+\n",
      "|      Pred | class             |\n",
      "|-----------+-------------------|\n",
      "| 0.67452   | Laws              |\n",
      "| 0.127029  | International     |\n",
      "| 0.0588022 | Ethics            |\n",
      "| 0.0436148 | Environment       |\n",
      "| 0.0308774 | US_Politics       |\n",
      "| 0.0181738 | News              |\n",
      "| 0.0178023 | (Social)_Media    |\n",
      "| 0.0155433 | US_Elections_2016 |\n",
      "| 0.0136375 | Guns              |\n",
      "+-----------+-------------------+\n",
      "{ ['off', 'the', 'table', 'the', 'dreamers', 'are', 'doomed', 'they', 'have', 'no', 'other', 'card', 'to', 'play', 'other', 'than', 'the', 'threat', 'of', 'non'] }\t predicted class: Laws\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non }\t sentiment score: -5.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non winning events }\t sentiment score: -1.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like }\t sentiment score: 1.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good }\t sentiment score: 4.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save }\t sentiment score: 6.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save the\n",
      "{ off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save the }\t sentiment score: 6.0 \n",
      "\n",
      "off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save the supreme court\n",
      "New sentence: { off the table the dreamers are doomed they have no other card to play other than the threat of non winning events like the good old days save the supreme court }\t score: 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['off',\n",
       " 'the',\n",
       " 'table',\n",
       " 'the',\n",
       " 'dreamers',\n",
       " 'are',\n",
       " 'doomed',\n",
       " 'they',\n",
       " 'have',\n",
       " 'no',\n",
       " 'other',\n",
       " 'card',\n",
       " 'to',\n",
       " 'play',\n",
       " 'other',\n",
       " 'than',\n",
       " 'the',\n",
       " 'threat',\n",
       " 'of',\n",
       " 'non',\n",
       " 'winning',\n",
       " 'events',\n",
       " 'like',\n",
       " 'the',\n",
       " 'good',\n",
       " 'old',\n",
       " 'days',\n",
       " 'save',\n",
       " 'the',\n",
       " 'supreme',\n",
       " 'court']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df           = the dataframe with less preprocessing\n",
    "model        = the best classification model\n",
    "length       = number of words that should be taken to inizialize the sentence\n",
    "comment_list = list of all comments from where to take the words\n",
    "alpha        = smoothing parameters to create an index which combines sentiment and frequence\n",
    "end_seq      = number of element at the end of the sentence that are taken as input for expanding the sentence itself\n",
    "n_match      = minimum number of following sequences found before reducing the end_seq\n",
    "method       = if 0 the function strictly selects part of sentence with not null sentiment\n",
    "               if 1 the function may add pertinent words with null sentiment\n",
    "\"\"\"\n",
    "\n",
    "final_function_light(df=data_newkey, model=clf, length=20, comment_list=whole_list, \n",
    "                     alpha=0.05, end_seq=8, n_match=5, method=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
